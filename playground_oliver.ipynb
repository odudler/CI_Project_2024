{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models import *\n",
    "from grid_search import *\n",
    "\n",
    "# df_test = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "# args = read_config('config_models.yaml')\n",
    "# model = SVDplusplus(args)\n",
    "# print(args.svdpp)\n",
    "# train, test = train_test_split(df, test_size=args.test_set_size, random_state=args.random_state)\n",
    "# model.train(df_train)\n",
    "# model.predict(df_test, output_file=\"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing temp.csv ...\n",
      "/Users/oliver/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2024-05-10 00:12:26.736392: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m args \u001b[38;5;241m=\u001b[39m read_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_models.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralCF(args)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(df_test, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mncf_output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ETH/Master/4. Semester/Computational Intelligence Lab/Project/CI_Project_2024/models.py:151\u001b[0m, in \u001b[0;36mNeuralCF.train\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    137\u001b[0m data \u001b[38;5;241m=\u001b[39m NCFDataset(train_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, test_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m                   overwrite_test_file_full\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_neg_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, n_neg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m model \u001b[38;5;241m=\u001b[39m NCF (\n\u001b[1;32m    140\u001b[0m     n_users\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_users, \n\u001b[1;32m    141\u001b[0m     n_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_movies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[1;32m    150\u001b[0m )       \n\u001b[0;32m--> 151\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/recommenders/models/ncf/ncf_singlenode.py:392\u001b[0m, in \u001b[0;36mNCF.fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    389\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# calculate loss and update NCF parameters\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_input, item_input, labels \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mtrain_loader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    394\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser2id[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m user_input])\n\u001b[1;32m    395\u001b[0m     item_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem2id[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m item_input])\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/recommenders/models/ncf/dataset.py:522\u001b[0m, in \u001b[0;36mDataset.train_loader\u001b[0;34m(self, batch_size, shuffle_size, yield_id, write_to)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_datafile \u001b[38;5;28;01mas\u001b[39;00m train_datafile:\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m train_datafile\u001b[38;5;241m.\u001b[39musers:\n\u001b[0;32m--> 522\u001b[0m         user_positive_examples \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datafile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m         user_positive_item_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    524\u001b[0m             user_positive_examples[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    525\u001b[0m         )\n\u001b[1;32m    526\u001b[0m         n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neg \u001b[38;5;241m*\u001b[39m user_positive_examples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/recommenders/models/ncf/dataset.py:211\u001b[0m, in \u001b[0;36mDataFile.load_data\u001b[0;34m(self, key, by_user)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/pandas/core/frame.py:2541\u001b[0m, in \u001b[0;36mDataFrame.from_records\u001b[0;34m(cls, data, index, exclude, columns, coerce_float, nrows)\u001b[0m\n\u001b[1;32m   2537\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_remove]\n\u001b[1;32m   2539\u001b[0m     columns \u001b[38;5;241m=\u001b[39m columns\u001b[38;5;241m.\u001b[39mdrop(exclude)\n\u001b[0;32m-> 2541\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43m_get_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode.data_manager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2542\u001b[0m mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(arrays, columns, result_index, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/pandas/_config/config.py:146\u001b[0m, in \u001b[0;36m_get_option\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_option\u001b[39m(pat: \u001b[38;5;28mstr\u001b[39m, silent: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 146\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43m_get_single_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;66;03m# walk the nested dict\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     root, k \u001b[38;5;241m=\u001b[39m _get_root(key)\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/pandas/_config/config.py:128\u001b[0m, in \u001b[0;36m_get_single_key\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_single_key\u001b[39m(pat: \u001b[38;5;28mstr\u001b[39m, silent: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[43m_select_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silent:\n",
      "File \u001b[0;32m~/mambaforge/envs/ci_project_3_9/lib/python3.9/site-packages/pandas/_config/config.py:617\u001b[0m, in \u001b[0;36m_select_options\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    610\u001b[0m     _deprecated_options[key] \u001b[38;5;241m=\u001b[39m DeprecatedOption(key, msg, rkey, removal_ver)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# functions internal to the module\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select_options\u001b[39m(pat: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    returns a list of keys matching `pat`\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    if pat==\"all\", returns all registered options\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# short-circuit for exact key\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/data_train.csv\")\n",
    "df_test = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "args = read_config('config_models.yaml')\n",
    "\n",
    "model = NeuralCF(args)\n",
    "\n",
    "model.train(df_train)\n",
    "model.predict(df_test, output_file='ncf_output.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/data_train.csv\")\n",
    "best_params, best_score = perform_grid_search(\"NeuralCF\", df_train, 'grid_search_args.yaml')\n",
    "print(best_params)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=10000, \n",
    "    n_items=1000,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(arr, n_row, n_col, imputation):\n",
    "    ### Column Normalization\n",
    "    masked = np.ma.masked_equal(arr, 0)\n",
    "    # to check: mean along row / col have effects on results?\n",
    "    mean_cols = np.tile(np.ma.mean(masked, axis=0).data, (n_row, 1))\n",
    "    std_cols = np.tile(np.ma.std(masked, axis=0).data, (n_row, 1))\n",
    "    normalized_arr = ((masked - mean_cols) / std_cols).data\n",
    "\n",
    "    ### Imputation\n",
    "    if imputation == \"zero\":\n",
    "        imputed_arr = normalized_arr\n",
    "    elif imputation == \"mean\":\n",
    "        imputed_arr = mean_cols * (normalized_arr == 0) + arr * (normalized_arr != 0)\n",
    "\n",
    "    return imputed_arr, mean_cols, std_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "\n",
    "df_train = pd.read_csv(\"data/data_train.csv\")\n",
    "users, movies, predictions = extract_users_items_predictions(df_train)\n",
    "\n",
    "data_matrix = create_data_matrix(users, movies, predictions)\n",
    "\n",
    "imp_arr, mean, std = preprocess(data_matrix, 10000, 1000, \"zero\")\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "predictions_real = imp_arr[users, movies]\n",
    "\n",
    "\n",
    "df_new = pd.DataFrame({\n",
    "    'itemID': movies,\n",
    "    'userID': users,\n",
    "    'rating': predictions_real\n",
    "})\n",
    "df_new.sort_values(by=['userID'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_new)\n",
    "df_new.to_csv('testing.csv')\n",
    "\n",
    "data = NCFDataset(train_file='testing.csv', test_file=None, seed=42, binary=False,\n",
    "                          overwrite_test_file_full=False, n_neg_test=0, n_neg=0)\n",
    "\n",
    "model.fit(data)\n",
    "\n",
    "print(model.predict(100,100))\n",
    "print(model.predict(200,200))\n",
    "print(model.predict(300,300))\n",
    "print(model.predict(400,400))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(raw_predictions, data_mean = None, data_std = None, min_rate = 1, max_rate = 5, denorm = True):\n",
    "    denormalized_predictions = raw_predictions\n",
    "    if denorm:\n",
    "        denormalized_predictions = raw_predictions * data_std + data_mean\n",
    "    clipped_predicted = np.clip(denormalized_predictions, min_rate, max_rate)\n",
    "    return clipped_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# predictions = [[row, col, model.predict(row, col)] for (row, col) in itertools.product(np.arange(10000), np.arange(1000))]\n",
    "\n",
    "# predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
    "# print(1)\n",
    "# reconstructed = np.zeros((10000, 1000))\n",
    "# all_pred = predictions.copy(deep=True)\n",
    "# print(2)\n",
    "# for ind, row in all_pred.iterrows():\n",
    "#     reconstructed[int(row.userID), int(row.itemID)] = row.prediction\n",
    "# print(3)\n",
    "reconstructed = np.zeros((10000, 1000))\n",
    "for i in range(10000):\n",
    "    if i % 100 == 0:\n",
    "        print(\"i\")\n",
    "    for j in range(1000):\n",
    "        reconstructed[i,j] = model.predict(i,j)\n",
    "print(3)\n",
    "reconstructed = postprocess(reconstructed, mean, std)\n",
    "print(4)\n",
    "df_test = pd.read_csv(\"data/sampleSubmission.csv\")\n",
    "users, movies, _ = extract_users_items_predictions(df_test)\n",
    "print(5)\n",
    "create_submission_from_matrix(reconstructed, users, movies, store_path=\"subnetmissionwork.csv\")\n",
    "\n",
    "print(model.predict(100,100))\n",
    "print(model.predict(200,200))\n",
    "print(model.predict(300,300))\n",
    "print(model.predict(400,400))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
